{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHdMfFcnf_Ki"
   },
   "source": [
    "## Logistic Regression Modeling for Early Stage Diabetes Risk Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1: Getting familiar with linear algebraic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "- Create matrix of size 10*10 with random integer numbers\n",
    "- Compute the following linear algebric operations on the matrix using built in functions supported in Numpy, Scipy etc.\n",
    "  - Find inverse of the matrix and print it\n",
    "  - Calculate dot product of the matrix with same matrix in transpose A.AT\n",
    "  - Decompose the original matrix using eigen decomposition print the eigen values and eigen vectors\n",
    "  - Calculate jacobian matrix \n",
    "  - Calculate hessian matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.random.rand(10,10)\n",
    "b=np.random.rand(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1317605 ,  0.14193943,  0.79549129, -1.67110839,  1.37963929,\n",
       "         0.22097196,  0.72410053, -0.13814585,  0.74768078, -2.01466235],\n",
       "       [-2.6095634 ,  1.08910692,  2.86399697, -1.69186905,  1.69437172,\n",
       "        -4.61221837, -0.08942897, -3.99738266,  5.5372264 ,  0.78206469],\n",
       "       [ 3.48647724, -0.98778431, -6.35195669,  3.72197042, -4.18540161,\n",
       "         7.92128811,  0.10916805,  6.52829342, -7.84885674, -0.24408697],\n",
       "       [ 0.25102139,  2.04742623,  0.22017065,  1.36310247, -0.42339381,\n",
       "        -2.85806564,  0.44590008, -1.49618119, -0.20510395,  0.72387142],\n",
       "       [-3.14375527, -0.36683319,  6.60714678, -3.5116636 ,  3.84495432,\n",
       "        -4.73589027,  0.01917466, -5.87619518,  6.17748668, -0.31237318],\n",
       "       [ 1.367251  ,  1.25181009, -1.02511043,  0.21253863, -1.21424674,\n",
       "        -0.74727493,  0.32493035,  0.91019968, -1.10749776,  0.30424712],\n",
       "       [ 0.39522702,  0.25192275, -0.73667275,  1.43858252, -0.77685174,\n",
       "         0.06631761,  1.25882701,  0.94746038, -2.80682849,  0.50014281],\n",
       "       [ 2.28101997,  1.95933829, -3.17992112,  5.3018073 , -4.97200625,\n",
       "        -2.73734908,  3.44398275, -1.01744094, -3.79304861,  2.79736054],\n",
       "       [-2.12618495, -3.00806202,  1.95455741, -4.63132548,  4.46792536,\n",
       "         3.33977961, -4.56980185,  1.01835971,  4.45099483, -1.06551701],\n",
       "       [-0.22421122, -1.81461326,  0.15613456, -0.49214472,  0.99638755,\n",
       "         2.61339176, -1.29188995,  1.88291976, -0.10886638, -1.06368465]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inva=np.linalg.inv(a)\n",
    "inva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.92268526, 2.21468371, 2.64379837, 2.50813295, 3.2027654 ,\n",
       "        2.54967053, 3.03416561, 2.44906083, 3.28739581, 3.61874842],\n",
       "       [2.21468371, 2.79522789, 2.54352739, 1.92083574, 2.14350538,\n",
       "        2.65231823, 2.52186469, 2.09470954, 2.58539002, 3.05679698],\n",
       "       [2.64379837, 2.54352739, 3.33617091, 2.24510413, 1.75064068,\n",
       "        2.50535896, 2.59602626, 2.81707382, 2.99802442, 3.10552769],\n",
       "       [2.50813295, 1.92083574, 2.24510413, 2.50849883, 2.36749776,\n",
       "        2.00806981, 2.29260818, 2.19606074, 2.73401356, 2.42974147],\n",
       "       [3.2027654 , 2.14350538, 1.75064068, 2.36749776, 3.86356988,\n",
       "        2.39351843, 3.10124717, 2.01407041, 2.78144278, 3.55019953],\n",
       "       [2.54967053, 2.65231823, 2.50535896, 2.00806981, 2.39351843,\n",
       "        2.78166386, 2.7427704 , 2.0150441 , 2.7432954 , 3.2665699 ],\n",
       "       [3.03416561, 2.52186469, 2.59602626, 2.29260818, 3.10124717,\n",
       "        2.7427704 , 3.56793614, 2.77294015, 3.28810705, 3.84751469],\n",
       "       [2.44906083, 2.09470954, 2.81707382, 2.19606074, 2.01407041,\n",
       "        2.0150441 , 2.77294015, 3.06810048, 2.99595593, 2.97492118],\n",
       "       [3.28739581, 2.58539002, 2.99802442, 2.73401356, 2.78144278,\n",
       "        2.7432954 , 3.28810705, 2.99595593, 3.63440491, 3.5454941 ],\n",
       "       [3.61874842, 3.05679698, 3.10552769, 2.42974147, 3.55019953,\n",
       "        3.2665699 , 3.84751469, 2.97492118, 3.5454941 , 4.73765197]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.19594821+0.j        , -0.60319215+0.19447981j,\n",
       "        -0.60319215-0.19447981j,  0.34452296+0.69284833j,\n",
       "         0.34452296-0.69284833j, -0.22348589+0.j        ,\n",
       "         0.32086014+0.24538349j,  0.32086014-0.24538349j,\n",
       "         0.08657808+0.16143308j,  0.08657808-0.16143308j]),\n",
       " array([[-0.35072544+0.j        ,  0.06814155-0.046351j  ,\n",
       "          0.06814155+0.046351j  ,  0.42187871-0.10514727j,\n",
       "          0.42187871+0.10514727j, -0.11144587+0.j        ,\n",
       "         -0.27103656+0.13348167j, -0.27103656-0.13348167j,\n",
       "         -0.09213904-0.01839121j, -0.09213904+0.01839121j],\n",
       "        [-0.26212671+0.j        ,  0.04880203-0.08720794j,\n",
       "          0.04880203+0.08720794j, -0.31398256-0.26116426j,\n",
       "         -0.31398256+0.26116426j,  0.33772409+0.j        ,\n",
       "         -0.18526849-0.32899665j, -0.18526849+0.32899665j,\n",
       "         -0.39012128-0.11332632j, -0.39012128+0.11332632j],\n",
       "        [-0.29091249+0.j        , -0.54378877+0.j        ,\n",
       "         -0.54378877-0.j        , -0.13168664-0.10931071j,\n",
       "         -0.13168664+0.10931071j, -0.52398226+0.j        ,\n",
       "          0.42978188+0.j        ,  0.42978188-0.j        ,\n",
       "          0.61942381+0.j        ,  0.61942381-0.j        ],\n",
       "        [-0.27320905+0.j        ,  0.01817866+0.15034595j,\n",
       "          0.01817866-0.15034595j,  0.25837613+0.05666604j,\n",
       "          0.25837613-0.05666604j, -0.01022131+0.j        ,\n",
       "         -0.00406399-0.23430459j, -0.00406399+0.23430459j,\n",
       "         -0.07044452-0.15457356j, -0.07044452+0.15457356j],\n",
       "        [-0.33493637+0.j        ,  0.4137533 -0.15619465j,\n",
       "          0.4137533 +0.15619465j,  0.48636248+0.j        ,\n",
       "          0.48636248-0.j        ,  0.66485353+0.j        ,\n",
       "         -0.23569328+0.3181712j , -0.23569328-0.3181712j ,\n",
       "         -0.42691171+0.21103689j, -0.42691171-0.21103689j],\n",
       "        [-0.28154996+0.j        ,  0.26861877+0.11193575j,\n",
       "          0.26861877-0.11193575j, -0.11190745-0.19460024j,\n",
       "         -0.11190745+0.19460024j, -0.10726528+0.j        ,\n",
       "         -0.01134219-0.1089006j , -0.01134219+0.1089006j ,\n",
       "          0.05624785-0.07651347j,  0.05624785+0.07651347j],\n",
       "        [-0.33661468+0.j        ,  0.08179484+0.10226953j,\n",
       "          0.08179484-0.10226953j, -0.07391414+0.10747675j,\n",
       "         -0.07391414-0.10747675j, -0.11429643+0.j        ,\n",
       "          0.15522553+0.3856615j ,  0.15522553-0.3856615j ,\n",
       "          0.14404988+0.0895588j ,  0.14404988-0.0895588j ],\n",
       "        [-0.28122198+0.j        , -0.51685791-0.06445607j,\n",
       "         -0.51685791+0.06445607j, -0.24073607+0.32256063j,\n",
       "         -0.24073607-0.32256063j,  0.27835455+0.j        ,\n",
       "          0.21469217-0.02044916j,  0.21469217+0.02044916j,\n",
       "          0.22929162-0.0969388j ,  0.22929162+0.0969388j ],\n",
       "        [-0.34195607+0.j        , -0.01609622+0.17656074j,\n",
       "         -0.01609622-0.17656074j, -0.09041232+0.15694286j,\n",
       "         -0.09041232-0.15694286j, -0.20062874+0.j        ,\n",
       "         -0.26228077-0.22271546j, -0.26228077+0.22271546j,\n",
       "         -0.25569553+0.03982579j, -0.25569553-0.03982579j],\n",
       "        [-0.38500135+0.j        ,  0.21169942-0.14003955j,\n",
       "          0.21169942+0.14003955j, -0.04376864+0.21032172j,\n",
       "         -0.04376864-0.21032172j, -0.12053006+0.j        ,\n",
       "          0.08829059+0.11058712j,  0.08829059-0.11058712j,\n",
       "          0.08639276+0.11230394j,  0.08639276-0.11230394j]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,v=np.linalg.eig(a)\n",
    "w,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Logistic Regression using newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LWrifqkf_Kj"
   },
   "source": [
    "### Logistic regression\n",
    "Logistic regression uses an equation as the representation, very much like linear regression.\n",
    "\n",
    "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
    "\n",
    "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
    "\n",
    "#### Dataset\n",
    "The dataset is available at <strong>\"data/diabetes_data.csv\"</strong> in the respective challenge's repo.<br>\n",
    "<strong>Original Source:</strong> http://archive.ics.uci.edu/ml/machine-learning-databases/00529/diabetes_data_upload.csv. The dataset just got released in July 2020.<br><br>\n",
    "\n",
    "#### Features (X)\n",
    "\n",
    "1. Age                - Values ranging from 16-90\n",
    "2. Gender             - Binary value (Male/Female)\n",
    "3. Polyuria           - Binary value (Yes/No)\n",
    "4. Polydipsia         - Binary value (Yes/No)\n",
    "5. sudden weight loss - Binary value (Yes/No)\n",
    "6. weakness           - Binary value (Yes/No)\n",
    "7. Polyphagia         - Binary value (Yes/No)\n",
    "8. Genital thrush     - Binary value (Yes/No)\n",
    "9. visual blurring    - Binary value (Yes/No)\n",
    "10. Itching           - Binary value (Yes/No)\n",
    "11. Irritability      - Binary value (Yes/No)\n",
    "12. delayed healing   - Binary value (Yes/No)\n",
    "13. partial paresis   - Binary value (Yes/No)\n",
    "14. muscle stiffness  - Binary value (Yes/No)\n",
    "15. Alopecia          - Binary value (Yes/No)\n",
    "16. Obesity           - Binary value (Yes/No)\n",
    "\n",
    "#### Output/Target target (Y) \n",
    "17. class - Binary class (Positive/Negative)\n",
    "\n",
    "#### Objective\n",
    "To learn logistic regression and practice handling of both numerical and categorical features\n",
    "\n",
    "#### Tasks\n",
    "- Download, load the data and print first 5 and last 5 rows\n",
    "- Transform categorical features into numerical features. Use label encoding or any other suitable preprocessing technique\n",
    "- Since the age feature is in larger range, age column can be normalized into smaller scale (like 0 to 1) using different methods such as scaling, standardizing or any other suitable preprocessing technique (Example - sklearn.preprocessing.MinMaxScaler class)\n",
    "- Define X matrix (independent features) and y vector (target feature)\n",
    "- Split the dataset into 60% for training and rest 40% for testing (sklearn.model_selection.train_test_split function)\n",
    "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
    "- Use the trained model to predict on testing set\n",
    "- Print 'Accuracy' obtained on the testing dataset i.e. (sklearn.metrics.accuracy_score function)\n",
    "\n",
    "#### Further fun (will not be evaluated)\n",
    "- Plot loss curve (Loss vs number of iterations)\n",
    "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
    "- Training model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
    "- Shuffling of training samples with different *random seed values* in the train_test_split function. Check the model error for the testing data for each setup.\n",
    "- Print other classification metrics such as:\n",
    "    - classification report (sklearn.metrics.classification_report),\n",
    "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
    "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
    "\n",
    "#### Helpful links\n",
    "- Scikit-learn documentation for logistic regression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- How Logistic Regression works: https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
    "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- Training testing splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-i4VgviHf_Kk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ooYDzG4SnErt"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from the source\n",
    "!wget _URL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqZrgW_if_Kq"
   },
   "outputs": [],
   "source": [
    "# NOTE: DO NOT CHANGE THE VARIABLE NAME(S) IN THIS CELL\n",
    "# Load the data\n",
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjCRzhp_f_Kw"
   },
   "outputs": [],
   "source": [
    "# Handle categorical/binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aNK0lA1f_Kz"
   },
   "outputs": [],
   "source": [
    "# Normalize the age feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqCVUtIUf_K3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Uc-BEzqf_K-"
   },
   "outputs": [],
   "source": [
    "# Define your X and y\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DIiMrIaajX-Q"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing here\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights):\n",
    "    '''Predict class for X.\n",
    "    For the given dataset, predicted vector has only values 0/1\n",
    "    Args:\n",
    "        X : Numpy array (num_samples, num_features)\n",
    "        weights : Model weights for logistic regression\n",
    "    Returns:\n",
    "        Binary predictions : (num_samples,)\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    z = \n",
    "    logits = \n",
    "    y_pred = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "        '''Sigmoid function: f:R->(0,1)\n",
    "        Args:\n",
    "            z : A numpy array (num_samples,)\n",
    "        Returns:\n",
    "            A numpy array where sigmoid function applied to every element\n",
    "        '''\n",
    "        ### START CODE HERE\n",
    "        sig_z = \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
    "        return sig_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    '''Calculate cross entropy loss\n",
    "    Note: Cross entropy is defined for multiple classes/labels as well\n",
    "    but for this dataset we only need binary cross entropy loss\n",
    "    Args:\n",
    "        y_true : Numpy array of true values (0/1) of size (num_samples,)\n",
    "        y_pred : Numpy array of predicted values (probabilites) of size (num_samples,)\n",
    "    Returns:\n",
    "        Cross entropy loss: A scalar value\n",
    "    '''\n",
    "    # Fix 0 values in y_pred\n",
    "    y_pred = np.maximum(np.full(y_pred.shape, 1e-7), np.minimum(np.full(y_pred.shape, 1-1e-7), y_pred))\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    ce_loss = \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_optimization(X, y, max_iterations=25):\n",
    "    '''Implement netwon method for optimizing weights\n",
    "    Args:\n",
    "        X : Numpy array (num_samples, num_features)\n",
    "        max_iterations : Max iterations to update the weights\n",
    "    Returns:\n",
    "        Optimal weights (num_features,)\n",
    "    '''\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    # Initialize random weights\n",
    "    weights = np.zeros(num_features,)\n",
    "    # Initialize losses\n",
    "    losses = []\n",
    "    \n",
    "    # Newton Method\n",
    "    for i in range(max_iterations):\n",
    "        # Predict/Calculate probabilties using sigmoid function\n",
    "        y_p = \n",
    "        \n",
    "        # Define gradient for J (cost function) i.e. cross entropy loss\n",
    "        gradient = \n",
    "        \n",
    "        # Define hessian matrix for cross entropy loss\n",
    "        hessian =\n",
    "        \n",
    "        # Update the model using hessian matrix and gradient computed\n",
    "        weights = \n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        loss = cross_entropy_loss(y, y_p)\n",
    "        # Append it\n",
    "        losses.append(loss)\n",
    "\n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train weights\n",
    "weights, losses = newton_optimization(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot([i+1 for i in range(len(losses))], losses)\n",
    "plt.title(\"Loss curve\")\n",
    "plt.xlabel(\"Iteration num\")\n",
    "plt.ylabel(\"Cross entropy curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model_test_acuracy = accuracy_score(y_test, predict(X_test, weights))\n",
    "\n",
    "print(f\"\\nAccuracy in testing set by our model: {our_model_test_acuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with the scikit learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qhvibx3Xf_LB"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(solver='newton-cg', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndXHgNLxf_LD"
   },
   "outputs": [],
   "source": [
    "# Fit the model. Wait! We will complete this step for you ;)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHOeLfjFjeNh"
   },
   "outputs": [],
   "source": [
    "# Predict on testing set X_test\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eE5g0uoYf_LG"
   },
   "outputs": [],
   "source": [
    "# Print Accuracy on testing set\n",
    "sklearn_test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy in testing set by sklearn model: {sklearn_test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task_2_logistic_diabetes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
