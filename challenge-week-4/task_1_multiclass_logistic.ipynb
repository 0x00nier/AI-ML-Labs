{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMHjtVPbyaKP"
   },
   "source": [
    "## Multinomial Logistic Regression Model for Obesity Level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdWC89WA80ne"
   },
   "source": [
    "## Part 1.1: Implement  multinomial logistic regression using softmax from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0agEvZVFeo5"
   },
   "source": [
    "### Logistic regression\n",
    "Logistic regression uses an equation as the representation, very much like linear regression.\n",
    "\n",
    "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
    "\n",
    "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Multiclass Logistic regression using Softmax\n",
    "\n",
    "The softmax function, also known as softargmax or normalized exponential function, is a generalization of the logistic function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural network.\n",
    "\n",
    "${\\displaystyle \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}{\\text{ for }}i=1,\\dotsc ,K{\\text{ and }}\\mathbf {z} =(z_{1},\\dotsc ,z_{K})\\in \\mathbb {R} ^{K}}$\n",
    "\n",
    "Here K is the number of class and each zi is calculated using \n",
    "\n",
    "$z_i = w_0 + w_1 * x_1 + ... + w_p * x_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJi26z8awmSD"
   },
   "source": [
    "\n",
    "#### Dataset\n",
    "The dataset is available at <strong>\"data/obesity_data.csv\"</strong> in the respective challenge's repo.<br>\n",
    "<strong>Original Source:</strong> https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+\n",
    "\n",
    "This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III. <br><br>\n",
    "\n",
    "\n",
    "#### Features (X)\n",
    "\n",
    "1. Gender {Female,Male}\n",
    "2. Age {numeric}\n",
    "3. Height {numeric}\n",
    "4. Weight {numeric}\n",
    "5. family_history_with_overweight: Has a family member suffered or suffers from overweight? {yes,no}\n",
    "6. FAVC : Do you eat high caloric food frequently? {yes,no}\n",
    "7. FCVC : Do you usually eat vegetables in your meals? {numeric}\n",
    "8. NCP : How many main meals do you have daily? {numeric}\n",
    "9. CAEC : Do you eat any food between meals? {no,Sometimes,Frequently,Always}\n",
    "10. SMOKE : Do you smoke? {yes,no}\n",
    "11. CH2O : How much water do you drink daily? {numeric}\n",
    "12. SCC : Do you monitor the calories you eat daily? {yes,no}\n",
    "13. FAF : How often do you have physical activity? {numeric}\n",
    "14. TUE : How much time do you use technological devices such as cell phone, videogames, television, computer and others? {numeric}\n",
    "15. CALC : how often do you drink alcohol? {no,Sometimes,Frequently,Always}\n",
    "16. MTRANS : Daily Transportation {Automobile,Motorbike,Bike, Public_Transportation,Walking}\n",
    "\n",
    "Take a look above at the source of the original dataset for more details.\n",
    "\n",
    "#### Target (y)\n",
    "17. NObeyesdad {Insufficient_Weight,Normal_Weight,Overweight_Level_I,Overweight_Level_II,Obesity_Type_I,Obesity_Type_II,Obesity_Type_III}\n",
    "\n",
    "#### Objective\n",
    "To gain understanding of multiclass classification using logistic regression through implementing the model from scratch\n",
    "\n",
    "#### Tasks\n",
    "- Download and load the data\n",
    "- Add intercept column with all values=1\n",
    "- Feature transformation:\n",
    "    - Convert 'Gender' column to numbers where 'Female' is 1 and 'Male' is 0\n",
    "    - Convert yes/no columns ['family_history_with_overweight','FAVC','SMOKE','SCC'] to 1/0\n",
    "    - One-Hot encode 'MTRANS', and 'NObeyesdad' columns. *Note:* One-hot encoding class/target variable is required for comparing binary predictions during training.\n",
    "    - Label encode 'CAEC', and 'CALC' columns\n",
    "    - Since the features have relatively different ranges, normalize the dataset\n",
    "- Define X matrix (independent features) and y matrix (target features) as numpy arrays\n",
    "- Print the shape and datatype of both X and y\n",
    "- Split the dataset into 80% for training and rest 20% for testing (sklearn.model_selection.train_test_split function)\n",
    "- Follow logistic regression class and fill code where highlighted:\n",
    "    - Write softmax function to predict probabilities for all classes\n",
    "    - Write cross entropy loss function\n",
    "    - Write fit function where gradient descent is implemented\n",
    "    - Write predict_proba function where we predict probabilities for input data\n",
    "    - Write predict function to select single class for given input from probabilities\n",
    "- Train the model\n",
    "- Write function for calculating accuracy\n",
    "- Compute accuracy on train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Fun (will not be evaluated)\n",
    "- Play with learning rate and max_iterations\n",
    "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
    "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
    "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
    "- Print other classification metrics such as:\n",
    "    - classification report (sklearn.metrics.classification_report),\n",
    "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
    "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
    "\n",
    "#### Helpful links\n",
    "- Multiclass Logistic Regression from scratch: https://gluon.mxnet.io/chapter02_supervised-learning/softmax-regression-scratch.html\n",
    "- Softmax tutorial : http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "- Softmax function detailed history: https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "- Understand gradients for cross entropy loss: https://rstudio-pubs-static.s3.amazonaws.com/337306_79a7966fad184532ab3ad66b322fe96e.html\n",
    "- OnevsRest (OVR) strategy for multiclass classification: https://medium.com/analytics-vidhya/logistic-regression-from-scratch-multi-classification-with-onevsall-d5c2acf0c37c\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21J6cpd_wmSE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4SL1fdNt1k3Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Users\\RAGHAV' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from the source\n",
    "!wget https://github.com/DeepConnectAI/challenge-week-4/raw/master/data/obesity_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9av7W-wowmSI"
   },
   "outputs": [],
   "source": [
    "# Read the data from local cloud directory\n",
    "data = pd.read_csv(\"data/obesity_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOtKXuWL80n4"
   },
   "outputs": [],
   "source": [
    "# Add the intercept column with all values 1\n",
    "data.insert(0,\"w0\",1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV1jGAQxwmSP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w0</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>87.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Walking</td>\n",
       "      <td>Overweight_Level_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>89.8</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w0  Gender   Age  Height  Weight family_history_with_overweight FAVC  FCVC  \\\n",
       "0   1  Female  21.0    1.62    64.0                            yes   no   2.0   \n",
       "1   1  Female  21.0    1.52    56.0                            yes   no   3.0   \n",
       "2   1    Male  23.0    1.80    77.0                            yes   no   2.0   \n",
       "3   1    Male  27.0    1.80    87.0                             no   no   3.0   \n",
       "4   1    Male  22.0    1.78    89.8                             no   no   2.0   \n",
       "\n",
       "   NCP       CAEC SMOKE  CH2O  SCC  FAF  TUE        CALC  \\\n",
       "0  3.0  Sometimes    no   2.0   no  0.0  1.0          no   \n",
       "1  3.0  Sometimes   yes   3.0  yes  3.0  0.0   Sometimes   \n",
       "2  3.0  Sometimes    no   2.0   no  2.0  1.0  Frequently   \n",
       "3  3.0  Sometimes    no   2.0   no  2.0  0.0  Frequently   \n",
       "4  1.0  Sometimes    no   2.0   no  0.0  0.0   Sometimes   \n",
       "\n",
       "                  MTRANS           NObeyesdad  \n",
       "0  Public_Transportation        Normal_Weight  \n",
       "1  Public_Transportation        Normal_Weight  \n",
       "2  Public_Transportation        Normal_Weight  \n",
       "3                Walking   Overweight_Level_I  \n",
       "4  Public_Transportation  Overweight_Level_II  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print some rows just to understand data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature transformation as per the above tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Gender\"]=data[\"Gender\"].apply(lambda x : 0 if x=='Male' else 1)\n",
    "data[\"Gender\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>SCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   family_history_with_overweight  FAVC  SMOKE  SCC\n",
       "0                               1     0      0    0\n",
       "1                               1     0      1    1\n",
       "2                               1     0      0    0\n",
       "3                               0     0      0    0\n",
       "4                               0     0      0    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_no=[c for c in data.select_dtypes(exclude=[np.number]).columns if data[c].str.contains(\"yes\").any()]\n",
    "data[yes_no]=data[yes_no].applymap(lambda x : 1 if x=='yes' else 0)\n",
    "data[yes_no].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w0</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Walking</td>\n",
       "      <td>Overweight_Level_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>89.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w0  Gender   Age  Height  Weight  family_history_with_overweight  FAVC  \\\n",
       "0   1       1  21.0    1.62    64.0                               1     0   \n",
       "1   1       1  21.0    1.52    56.0                               1     0   \n",
       "2   1       0  23.0    1.80    77.0                               1     0   \n",
       "3   1       0  27.0    1.80    87.0                               0     0   \n",
       "4   1       0  22.0    1.78    89.8                               0     0   \n",
       "\n",
       "   FCVC  NCP       CAEC  SMOKE  CH2O  SCC  FAF  TUE        CALC  \\\n",
       "0   2.0  3.0  Sometimes      0   2.0    0  0.0  1.0          no   \n",
       "1   3.0  3.0  Sometimes      1   3.0    1  3.0  0.0   Sometimes   \n",
       "2   2.0  3.0  Sometimes      0   2.0    0  2.0  1.0  Frequently   \n",
       "3   3.0  3.0  Sometimes      0   2.0    0  2.0  0.0  Frequently   \n",
       "4   2.0  1.0  Sometimes      0   2.0    0  0.0  0.0   Sometimes   \n",
       "\n",
       "                  MTRANS           NObeyesdad  \n",
       "0  Public_Transportation        Normal_Weight  \n",
       "1  Public_Transportation        Normal_Weight  \n",
       "2  Public_Transportation        Normal_Weight  \n",
       "3                Walking   Overweight_Level_I  \n",
       "4  Public_Transportation  Overweight_Level_II  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w0</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>...</th>\n",
       "      <th>Motorbike</th>\n",
       "      <th>Public_Transportation</th>\n",
       "      <th>Walking</th>\n",
       "      <th>Insufficient_Weight</th>\n",
       "      <th>Normal_Weight</th>\n",
       "      <th>Obesity_Type_I</th>\n",
       "      <th>Obesity_Type_II</th>\n",
       "      <th>Obesity_Type_III</th>\n",
       "      <th>Overweight_Level_I</th>\n",
       "      <th>Overweight_Level_II</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>89.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   w0  Gender   Age  Height  Weight  family_history_with_overweight  FAVC  \\\n",
       "0   1       1  21.0    1.62    64.0                               1     0   \n",
       "1   1       1  21.0    1.52    56.0                               1     0   \n",
       "2   1       0  23.0    1.80    77.0                               1     0   \n",
       "3   1       0  27.0    1.80    87.0                               0     0   \n",
       "4   1       0  22.0    1.78    89.8                               0     0   \n",
       "\n",
       "   FCVC  NCP       CAEC  ...  Motorbike  Public_Transportation  Walking  \\\n",
       "0   2.0  3.0  Sometimes  ...          0                      1        0   \n",
       "1   3.0  3.0  Sometimes  ...          0                      1        0   \n",
       "2   2.0  3.0  Sometimes  ...          0                      1        0   \n",
       "3   3.0  3.0  Sometimes  ...          0                      0        1   \n",
       "4   2.0  1.0  Sometimes  ...          0                      1        0   \n",
       "\n",
       "   Insufficient_Weight  Normal_Weight Obesity_Type_I  Obesity_Type_II  \\\n",
       "0                    0              1              0                0   \n",
       "1                    0              1              0                0   \n",
       "2                    0              1              0                0   \n",
       "3                    0              0              0                0   \n",
       "4                    0              0              0                0   \n",
       "\n",
       "   Obesity_Type_III  Overweight_Level_I  Overweight_Level_II  \n",
       "0                 0                   0                    0  \n",
       "1                 0                   0                    0  \n",
       "2                 0                   0                    0  \n",
       "3                 0                   1                    0  \n",
       "4                 0                   0                    1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrans=pd.get_dummies(data.MTRANS)\n",
    "nobeyesdad=pd.get_dummies(data.NObeyesdad)\n",
    "data=data.drop(columns=['MTRANS','NObeyesdad'])\n",
    "data=data.join(mtrans).join(nobeyesdad)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Insufficient_Weight',\n",
       " 'Normal_Weight',\n",
       " 'Obesity_Type_I',\n",
       " 'Obesity_Type_II',\n",
       " 'Obesity_Type_III',\n",
       " 'Overweight_Level_I',\n",
       " 'Overweight_Level_II']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nobeyesdad.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CAEC</th>\n",
       "      <th>CALC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CAEC  CALC\n",
       "0     2     3\n",
       "1     2     2\n",
       "2     2     1\n",
       "3     2     1\n",
       "4     2     2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "data[['CAEC','CALC']]=data[['CAEC','CALC']].apply(le.fit_transform)\n",
    "data[['CAEC','CALC']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joRU6dWxwmSR"
   },
   "outputs": [],
   "source": [
    "# Define X (input features) and y (output feature) \n",
    "X = data.drop(columns=['Insufficient_Weight',\n",
    " 'Normal_Weight',\n",
    " 'Obesity_Type_I',\n",
    " 'Obesity_Type_II',\n",
    " 'Obesity_Type_III',\n",
    " 'Overweight_Level_I',\n",
    " 'Overweight_Level_II'])\n",
    "y = data[['Insufficient_Weight',\n",
    " 'Normal_Weight',\n",
    " 'Obesity_Type_I',\n",
    " 'Obesity_Type_II',\n",
    " 'Obesity_Type_III',\n",
    " 'Overweight_Level_I',\n",
    " 'Overweight_Level_II']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['w0', 'Gender', 'Age', 'Height', 'Weight',\n",
       "       'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC',\n",
       "       'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'Automobile', 'Bike',\n",
       "       'Motorbike', 'Public_Transportation', 'Walking'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w0</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>...</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>Automobile</th>\n",
       "      <th>Bike</th>\n",
       "      <th>Motorbike</th>\n",
       "      <th>Public_Transportation</th>\n",
       "      <th>Walking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>89.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   w0  Gender   Age  Height  Weight  family_history_with_overweight  FAVC  \\\n",
       "0   1       1  21.0    1.62    64.0                               1     0   \n",
       "1   1       1  21.0    1.52    56.0                               1     0   \n",
       "2   1       0  23.0    1.80    77.0                               1     0   \n",
       "3   1       0  27.0    1.80    87.0                               0     0   \n",
       "4   1       0  22.0    1.78    89.8                               0     0   \n",
       "\n",
       "   FCVC  NCP  CAEC  ...  CH2O  SCC  FAF  TUE  CALC  Automobile  Bike  \\\n",
       "0   2.0  3.0     2  ...   2.0    0  0.0  1.0     3           0     0   \n",
       "1   3.0  3.0     2  ...   3.0    1  3.0  0.0     2           0     0   \n",
       "2   2.0  3.0     2  ...   2.0    0  2.0  1.0     1           0     0   \n",
       "3   3.0  3.0     2  ...   2.0    0  2.0  0.0     1           0     0   \n",
       "4   2.0  1.0     2  ...   2.0    0  0.0  0.0     2           0     0   \n",
       "\n",
       "   Motorbike  Public_Transportation  Walking  \n",
       "0          0                      1        0  \n",
       "1          0                      1        0  \n",
       "2          0                      1        0  \n",
       "3          0                      0        1  \n",
       "4          0                      1        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"family_history_with_overweight\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "X[[\"Age\",\"Weight\",\"Height\",\"FCVC\",\"NCP\",\n",
    "   \"CAEC\",\"CH2O\",\"FAF\",\n",
    "   \"TUE\",\"CALC\"]]=scaler.fit_transform(X[[\"Age\",\"Weight\",\"Height\",\"FCVC\",\"NCP\",\"CAEC\",\"CH2O\",\"FAF\",\"TUE\",\"CALC\"]])\n",
    "y=y.astype('float64').values\n",
    "X=X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAyM-CYCwmSU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: Type-<class 'numpy.ndarray'>, Shape-(2111, 21)\n",
      "y: Type-<class 'numpy.ndarray'>, Shape-(2111, 7)\n"
     ]
    }
   ],
   "source": [
    "X_shape = X.shape\n",
    "X_type  = type(X)\n",
    "y_shape = y.shape\n",
    "y_type  = type(y)\n",
    "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
    "print(f'y: Type-{y_type}, Shape-{y_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6W4_2vt80oI"
   },
   "source": [
    "Shape of X will depend on how feature transformation is done, but number of columns should be >=17, and there should be 2111 rows.\n",
    "\n",
    "Nevertheless, type of X must be <class 'numpy.ndarray'>\n",
    "\n",
    "<strong>Output for y: </strong><br>\n",
    "\n",
    "y: Type-<class 'numpy.ndarray'>, Shape-(2111,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8WF-EqO3BEa"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing here\n",
    "# Use RNADOM STATE parameter as well to reproduce results later\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acCATJhI3FdH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1688, 21) , y_train: (1688, 7)\n",
      "X_test: (423, 21) , y_test: (423, 7)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of features and target of training and testing: X_train, X_test, y_train, y_test\n",
    "X_train_shape = X_train.shape\n",
    "y_train_shape = y_train.shape\n",
    "X_test_shape  = X_test.shape\n",
    "y_test_shape  = y_test.shape\n",
    "\n",
    "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
    "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
    "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSa7cW-NwmSd"
   },
   "source": [
    "##### Let us start implementing logistic regression from scratch. Just follow code cells, see hints if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tuy_qkR280oY"
   },
   "source": [
    "##### We will build a LogisticRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23RZbK3P80oZ"
   },
   "outputs": [],
   "source": [
    "# DO NOT EDIT ANY VARIABLE OR FUNCTION NAME(S) IN THIS CELL\n",
    "# Let's try more object oriented approach this time :)\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        '''Initialize variables\n",
    "        Args:\n",
    "            learning_rate  : Learning Rate\n",
    "            max_iterations : Max iterations for training weights\n",
    "        '''\n",
    "        # Initialising all the parameters\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.losses    = []\n",
    "        \n",
    "        # Define epsilon because log(0) is not defined\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def softmax(self, z):\n",
    "        '''Softmax function\n",
    "        Args:\n",
    "            z : A numpy array (num_samples,num_classes)\n",
    "        Returns:\n",
    "            A numpy array where softmax function applied to every sample\n",
    "        '''\n",
    "        assert len(z.shape) == 2\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        z = np.maximum(np.full(z.shape, self.eps), np.minimum(np.full(z.shape, 1-self.eps), z))\n",
    "        exp=np.exp(z)\n",
    "        soft_z = np.divide(exp,np.sum(exp))\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return soft_z\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        '''Compute cross_entropy_loss\n",
    "        Args:\n",
    "            y_true : Numpy array of actual truth values (num_samples,num_classes)\n",
    "            y_pred : Numpy array of predicted values (num_samples,num_classes)\n",
    "        Returns:\n",
    "            Cross-entropy loss, scalar value (sum of cross entropy loss of individual classes)\n",
    "        '''\n",
    "        # Fix 0/1 values in y_pred so that log is not undefined\n",
    "        y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        # HINT: Take sum of losses of all classes\n",
    "        ce_loss = (1/y_true.size)*np.sum(-y_true*np.log(y_pred) - (1-y_true)*np.log(1-y_pred))\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return ce_loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Trains logistic regression model using gradient ascent\n",
    "        to gain maximum likelihood on the training data\n",
    "        Args:\n",
    "            X : Numpy array (num_examples, num_features)\n",
    "            y : Numpy array (num_examples, num_classes)\n",
    "        Returns: VOID\n",
    "        '''\n",
    "\n",
    "        num_examples = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        num_classes  = y.shape[1]\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        # Initialize weights with appropriate shape [num_features, num_classes]\n",
    "        self.weights = np.random.rand(num_features,num_classes)\n",
    "        \n",
    "        # Perform gradient ascent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Define the linear hypothesis(z) first\n",
    "            z=np.dot(X,self.weights)\n",
    "            # Output probability values using softmax\n",
    "            y_pred = self.softmax(z)\n",
    "            \n",
    "            # Compute gradient for weights assiciated with each class \n",
    "            gradient = np.dot(X.T,(y_pred -  y))\n",
    "            \n",
    "            # Update the weights\n",
    "            # Perform weight updation for each class\n",
    "            self.weights = self.weights+(self.learning_rate*gradient)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.cross_entropy_loss(y,y_pred)\n",
    "\n",
    "            self.losses.append(loss)\n",
    "    \n",
    "        ### END CODE HERE\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''Predict probabilities for given X.\n",
    "        Remember sigmoid returns value between 0 and 1.\n",
    "        Args:\n",
    "            X : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            probabilities: Numpy array (num_samples,num_classes)\n",
    "        '''\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Fit the model before prediction\")\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        z = np.dot(X,self.weights)\n",
    "        probabilities = self.softmax(z)\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Predict/Classify X in classes\n",
    "        Args:\n",
    "            X         : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            binary_predictions : Numpy array (num_samples, num_classes)\n",
    "        '''\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        # HINT: Choose maximum probability (Set value to 1 and rest 0) to predict binary values using np.argmax() function\n",
    "        pred=model.predict_proba(X)\n",
    "        m = np.zeros_like(pred)\n",
    "        m[np.arange(len(pred)), pred.argmax(1)] = 1\n",
    "        binary_predictions = m\n",
    "        \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpOjju_480oc"
   },
   "outputs": [],
   "source": [
    "# Now initialize multinomial logitic regression implemented by you\n",
    "model = MyLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3hevcFF80oe"
   },
   "outputs": [],
   "source": [
    "# And now fit on training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00079365, 0.00079365, 0.00079365, ..., 0.00079365, 0.00079365,\n",
       "       0.00079365])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/y_train.size)*np.sum(-y_train*np.log(model.predict_proba(X_train)) - (1-y_train)*np.log(1-model.predict_proba(X_train)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oyz8ev780oj"
   },
   "source": [
    "##### Phew!! That's a lot of code. But you did it, congrats !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tvMc0OqwmSp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on training data: 1.3396739473932089\n"
     ]
    }
   ],
   "source": [
    "# Train log-likelihood\n",
    "train_loss = model.cross_entropy_loss(y_train, model.predict_proba(X_train))\n",
    "print(\"Loss on training data:\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZQ8ITUt4b0N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on testing data: 1.142187001828929\n"
     ]
    }
   ],
   "source": [
    "# Test log-likelihood\n",
    "test_loss = model.cross_entropy_loss(y_test, model.predict_proba(X_test))\n",
    "print(\"Loss on testing data:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089,\n",
       " 1.3396739473932089]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPZ6w2IK80oq"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot([i+1 for i in range(len(model.losses))], model.losses)\n",
    "plt.title(\"Loss curve\")\n",
    "plt.xlabel(\"Iteration num\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_PXpSyC80ov"
   },
   "source": [
    "##### Let's calculate accuracy as well. Accuracy is defined simply as the rate of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDow5HNv80oy"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    '''Compute accuracy.\n",
    "    Accuracy = (Correct prediction / number of samples)\n",
    "    Args:\n",
    "        y_true : Truth binary values (num_examples, num_classes)\n",
    "        y_pred : Predicted binary values (num_examples, num_classes)\n",
    "    Returns:\n",
    "        accuracy: scalar value\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    accuracy = np.sum(y_true==y_pred)/y_true.size\n",
    "    ### END CODE HERE\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPlTLEDw80o2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7508463100880163\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy on train data\n",
    "print(accuracy(y_train,model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dM97OsR80o4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7521107733873691\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy on test data\n",
    "print(accuracy(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otuidf-v80o7"
   },
   "source": [
    "## Part 1.2: Use Logistic Regression from sklearn on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lecvVzu80o7"
   },
   "source": [
    "#### Tasks\n",
    "- Define X and y again for sklearn Linear Regression model\n",
    "- <strong>Note:</strong>\n",
    "    - Column at position 0 with all values=1 is not required. (Handled by scikit-learn built-in class)\n",
    "    - One-hot encoding of the target column is not required. (Handled by scikit-learn built-in class)\n",
    "    - Don't scale/normalize the target column, let them be whole numbers 0,1,2,... (sklearn does not recognize continuous values as categories)\n",
    "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
    "- Run the model on testing set\n",
    "- Print 'accuracy' obtained on the testing dataset (sklearn.metrics.accuracy_score function)\n",
    "\n",
    "#### Further fun (will not be evaluated)\n",
    "- Compare accuracies of your model and sklearn's logistic regression model\n",
    "\n",
    "#### Helpful links\n",
    "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-T6e7_o80o8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHqyjdEsL3Yv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w0</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>...</th>\n",
       "      <th>Motorbike</th>\n",
       "      <th>Public_Transportation</th>\n",
       "      <th>Walking</th>\n",
       "      <th>Insufficient_Weight</th>\n",
       "      <th>Normal_Weight</th>\n",
       "      <th>Obesity_Type_I</th>\n",
       "      <th>Obesity_Type_II</th>\n",
       "      <th>Obesity_Type_III</th>\n",
       "      <th>Overweight_Level_I</th>\n",
       "      <th>Overweight_Level_II</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.186567</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   w0  Gender       Age    Height    Weight  family_history_with_overweight  \\\n",
       "0   1       1  0.148936  0.320755  0.186567                               1   \n",
       "1   1       1  0.148936  0.132075  0.126866                               1   \n",
       "2   1       0  0.191489  0.660377  0.283582                               1   \n",
       "3   1       0  0.276596  0.660377  0.358209                               0   \n",
       "4   1       0  0.170213  0.622642  0.379104                               0   \n",
       "\n",
       "   FAVC  FCVC       NCP      CAEC  ...  Motorbike  Public_Transportation  \\\n",
       "0     0   0.5  0.666667  0.666667  ...          0                      1   \n",
       "1     0   1.0  0.666667  0.666667  ...          0                      1   \n",
       "2     0   0.5  0.666667  0.666667  ...          0                      1   \n",
       "3     0   1.0  0.666667  0.666667  ...          0                      0   \n",
       "4     0   0.5  0.000000  0.666667  ...          0                      1   \n",
       "\n",
       "   Walking  Insufficient_Weight  Normal_Weight  Obesity_Type_I  \\\n",
       "0        0                    0              1               0   \n",
       "1        0                    0              1               0   \n",
       "2        0                    0              1               0   \n",
       "3        1                    0              0               0   \n",
       "4        0                    0              0               0   \n",
       "\n",
       "   Obesity_Type_II  Obesity_Type_III  Overweight_Level_I  Overweight_Level_II  \n",
       "0                0                 0                   0                    0  \n",
       "1                0                 0                   0                    0  \n",
       "2                0                 0                   0                    0  \n",
       "3                0                 0                   1                    0  \n",
       "4                0                 0                   0                    1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform data loading and preprocessing here suitable for sklearn model\n",
    "# See above note in tasks to minimize implementation errors\n",
    "data.head()\n",
    "scaler=MinMaxScaler()\n",
    "data[[\"Age\",\"Weight\",\"Height\",\"FCVC\",\"NCP\",\n",
    "   \"CAEC\",\"CH2O\",\"FAF\",\n",
    "   \"TUE\",\"CALC\"]]=scaler.fit_transform(data[[\"Age\",\"Weight\",\"Height\",\"FCVC\",\"NCP\",\"CAEC\",\"CH2O\",\"FAF\",\"TUE\",\"CALC\"]])\n",
    "#data=data.drop(columns=[\"w0\"])\n",
    "y=data[['Insufficient_Weight',\n",
    " 'Normal_Weight',\n",
    " 'Obesity_Type_I',\n",
    " 'Obesity_Type_II',\n",
    " 'Obesity_Type_III',\n",
    " 'Overweight_Level_I',\n",
    " 'Overweight_Level_II']]\n",
    "y.insert(0, 'ID', range(0, len(y)))\n",
    "y.set_index('ID',inplace=True)\n",
    "y=y[y==1].stack().reset_index().drop(0,1)\n",
    "y=y.drop(columns=['ID'])\n",
    "le=LabelEncoder()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w0</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>Automobile</th>\n",
       "      <th>Bike</th>\n",
       "      <th>Motorbike</th>\n",
       "      <th>Public_Transportation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.186567</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w0  Gender       Age    Height    Weight  family_history_with_overweight  \\\n",
       "0   1       1  0.148936  0.320755  0.186567                               1   \n",
       "1   1       1  0.148936  0.132075  0.126866                               1   \n",
       "2   1       0  0.191489  0.660377  0.283582                               1   \n",
       "3   1       0  0.276596  0.660377  0.358209                               0   \n",
       "4   1       0  0.170213  0.622642  0.379104                               0   \n",
       "\n",
       "   FAVC  FCVC       NCP      CAEC  SMOKE  CH2O  SCC       FAF  TUE      CALC  \\\n",
       "0     0   0.5  0.666667  0.666667      0   0.5    0  0.000000  0.5  1.000000   \n",
       "1     0   1.0  0.666667  0.666667      1   1.0    1  1.000000  0.0  0.666667   \n",
       "2     0   0.5  0.666667  0.666667      0   0.5    0  0.666667  0.5  0.333333   \n",
       "3     0   1.0  0.666667  0.666667      0   0.5    0  0.666667  0.0  0.333333   \n",
       "4     0   0.5  0.000000  0.666667      0   0.5    0  0.000000  0.0  0.666667   \n",
       "\n",
       "   Automobile  Bike  Motorbike  Public_Transportation  \n",
       "0           0     0          0                      1  \n",
       "1           0     0          0                      1  \n",
       "2           0     0          0                      1  \n",
       "3           0     0          0                      0  \n",
       "4           0     0          0                      1  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[list(data.columns[0:20])].loc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oc4hFVS_80o-"
   },
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X_s = data.drop(columns=['Insufficient_Weight',\n",
    " 'Normal_Weight',\n",
    " 'Obesity_Type_I',\n",
    " 'Obesity_Type_II',\n",
    " 'Obesity_Type_III',\n",
    " 'Overweight_Level_I',\n",
    " 'Overweight_Level_II']).values\n",
    "y_s = le.fit_transform(y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  1\n",
       "2  1\n",
       "3  5\n",
       "4  6"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_s).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SAME TEST SIZE AND RANDOM STATE as above splitting to compare right\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_s, y_s, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrXCCTOQ80pF"
   },
   "outputs": [],
   "source": [
    "# Initialize the model from sklearn\n",
    "sk_model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7Xa-l7m80qC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "sk_model.fit(X_s_train, y_s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rCQvgSp80qH"
   },
   "outputs": [],
   "source": [
    "# Predict on testing set X_test\n",
    "y_s_pred = sk_model.predict(X_s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4K1rMC-80qL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on testing set: 0.7281323877068558\n"
     ]
    }
   ],
   "source": [
    "# Print Accuracy on testing set\n",
    "test_accuracy_sklearn = accuracy_score(y_s_test,y_s_pred)\n",
    "\n",
    "print(f\"\\nAccuracy on testing set: {test_accuracy_sklearn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUG9Wk_880qN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task_1_multiclass_logistic_obesity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
